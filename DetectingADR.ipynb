{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d1da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flair --quiet\n",
    "!pip install \"flair[word-embeddings]\"\n",
    "!pip install biopython\n",
    "!pip install spacy\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9946a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "import string\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional\n",
    "import re\n",
    "from Bio import Entrez\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cos_sim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import (\n",
    "    WordEmbeddings,\n",
    "    FlairEmbeddings,\n",
    "    StackedEmbeddings,\n",
    ")\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ff095f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid SSL certificate verification issues\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
    "getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a5f78d",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680386df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation_metrics(pred_drug: List[List[str]], true_drug: List[List[str]], pred_adr: List[List[str]], true_adr: List[List[str]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1 score for predicted drug/ADRs.\n",
    "    \"\"\"\n",
    "    # Drug/adr metrics\n",
    "    drug_tp = 0\n",
    "    drug_fp = 0\n",
    "    drug_fn = 0\n",
    "    adr_tp = 0\n",
    "    adr_fp = 0\n",
    "    adr_fn = 0\n",
    "    \n",
    "    for prd, ad, pa, aa in zip(pred_drug, true_drug, pred_adr, true_adr):\n",
    "        # Drug metrics\n",
    "        if len(prd) > 0:\n",
    "            for d in prd:\n",
    "                for a in ad:\n",
    "                    if d == a:\n",
    "                        drug_tp += 1\n",
    "                    else:\n",
    "                        drug_fp += 1\n",
    "        else:\n",
    "            drug_fn += len(ad)\n",
    "        \n",
    "        # ADR metrics\n",
    "        if len(prd) > 0:\n",
    "            for d in pa:\n",
    "                for a in aa:\n",
    "                    if d == a:\n",
    "                        adr_tp += 1\n",
    "                    else:\n",
    "                        adr_fp += 1\n",
    "        else:\n",
    "            adr_fn += len(ad)\n",
    "\n",
    "    # Drug and ADR metrics\n",
    "    precision_drug = drug_tp / (drug_tp + drug_fp)\n",
    "    recall_drug = drug_tp / (drug_tp + drug_fn)\n",
    "    f1_drug = 2 * (precision_drug * recall_drug) / (precision_drug + recall_drug)\n",
    "\n",
    "    precision_adr = adr_tp / (adr_tp + adr_fp)\n",
    "    recall_adr = adr_tp / (adr_tp + adr_fn)\n",
    "    f1_adr = 2 * (precision_adr * recall_adr) / (precision_adr + recall_adr)\n",
    "\n",
    "    # Overall metrics\n",
    "    true_positives = drug_tp + adr_tp\n",
    "    false_positives = drug_fp + adr_fp\n",
    "    false_negatives = drug_fn + adr_fn\n",
    "\n",
    "    precision = true_positives / (true_positives+false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    # Return results\n",
    "    results = pd.DataFrame({\"Metric\": [\"drug\", \"adr\", \"overall\"],\n",
    "                            \"Precision\": [precision_drug, precision_adr, precision],\n",
    "                            \"Recall\": [recall_drug, recall_adr, recall],\n",
    "                            \"F1 Score\": [f1_drug, f1_adr, f1]})\n",
    "\n",
    "    return results\n",
    "\n",
    "\"\"\"Sequence tag functions\"\"\"\n",
    "def clean_string(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns a cleaned version of input string:\n",
    "    - Strips leading/trailing whitespace\n",
    "    - Removes leading/trailing punctuation\n",
    "    - Converts to lowercase\n",
    "    - Splits hyphenated multiwords into two words\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(text, str):\n",
    "        # Remove leading/trailing whitespace\n",
    "        text = text.strip()\n",
    "\n",
    "        # Remove leading/trailing punctuation\n",
    "        text = text.strip(string.punctuation)\n",
    "\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def _get_entity_spans(entity: str, text: str) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Returns start and end indexes of entity occurence in text.\n",
    "    \"\"\"\n",
    "    literal_str = re.escape(entity)\n",
    "    matched_results = re.finditer(literal_str, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return [m.span() for m in matched_results]\n",
    "\n",
    "def sequence_tag_text(drugname: str, adr: str, text: str, drug_spans: Optional[List[Tuple[int, int]]] = None, adr_spans: Optional[List[Tuple[int, int]]] = None, detect_spans: bool = True) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Sequence tag a input text with IOB tags for drugname and ADR entities.\n",
    "    Optionally switch off span detection and provide spans directly.\n",
    "\n",
    "    The following tags are generated:\n",
    "    - B-DRUG: Beginning of Drug entity\n",
    "    - I-DRUG: Inside Drug entity\n",
    "    - B-ADR: Beginning of ADR entity\n",
    "    - I-ADR: Inside ADR entity\n",
    "    - O: Outside any entity\n",
    "    \"\"\"\n",
    "    sentence = Sentence(text, use_tokenizer=True)\n",
    "\n",
    "    # Initialize all tags as Others first\n",
    "    tags = ['O'] * len(sentence)\n",
    "\n",
    "    # Detect spans if not provided\n",
    "    if detect_spans:\n",
    "        drug_spans = _get_entity_spans(drugname, text)\n",
    "        adr_spans = _get_entity_spans(adr, text)\n",
    "\n",
    "    # IOB tag drug entities\n",
    "    for start, end in drug_spans:\n",
    "        for i, w in enumerate(sentence):\n",
    "            w_start, w_end = w.start_position, w.end_position\n",
    "\n",
    "            if w_start >= start and w_end <= end:\n",
    "                # Mark first word as Begining of Drug entity\n",
    "                if w_start == start:\n",
    "                    tags[i] = 'B-DRUG'\n",
    "                \n",
    "                # Subsequent words are Inside Drug spans\n",
    "                else:\n",
    "                    tags[i] = 'I-DRUG'\n",
    "\n",
    "    # IOB tag ADR entities\n",
    "    for start, end in adr_spans:\n",
    "        for i, w in enumerate(sentence):\n",
    "            w_start, w_end = w.start_position, w.end_position\n",
    "\n",
    "            if w_start >= start and w_end <= end:\n",
    "                # Mark first word as Begining of ADR entity\n",
    "                if w_start == start:\n",
    "                    tags[i] = 'B-ADR'\n",
    "                \n",
    "                # Subsequent words are Inside ADR spans\n",
    "                else:\n",
    "                    tags[i] = 'I-ADR'\n",
    "    \n",
    "    # Remove any I- tags that are preceded by O tags\n",
    "    preceeding_tag = 'B-placeholder'\n",
    "    preceeding_index = -1\n",
    "    for tag in tags:\n",
    "        if tag.startswith('I-'):\n",
    "            if preceeding_tag == 'O':\n",
    "                tags[preceeding_index+1] = 'O'\n",
    "        \n",
    "        preceeding_tag = tag\n",
    "        preceeding_index += 1\n",
    "\n",
    "    return list(zip([token.text for token in sentence], tags))\n",
    "\n",
    "def save_sequence_tagged_data(sequence_tagged_pairs: List[List[Tuple[str, str]]], output_dir: str, output_filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Save sequence tagged data to column-formatted txt output file.\n",
    "    Each line contains a word token and its corresponding tag.\n",
    "    \"\"\"\n",
    "    output_filepath = os.path.join(output_dir, output_filename)\n",
    "    with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "        for sentence in sequence_tagged_pairs:\n",
    "            for w, tag in sentence:\n",
    "                f.write(f\"{w} {tag}\\n\")\n",
    "        \n",
    "            # Blank line between sentences\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "\"\"\"Cosine Similarity functions\"\"\"\n",
    "def get_mean_word_vector(phrase: str, wordVector_model) -> list:\n",
    "    words = phrase.split()\n",
    "    vectors = [wordVector_model.wv[word] for word in words if word in wordVector_model.wv]\n",
    "    \"\"\"\n",
    "    Compute cosine similarity using mean of all word vectors in multi-word terms.\n",
    "    \"\"\"\n",
    "    if vectors:\n",
    "        mean_vector = np.mean(vectors, axis=0)\n",
    "        return mean_vector\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_pair_cosine_similarity(drug: str, effect: str, wordVector_model) -> float:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between drug and effect terms using pretrained word vectors.\n",
    "    \"\"\"\n",
    "    # Get mean of word vectors for multi-word terms\n",
    "    drug_vector = get_mean_word_vector(drug, wordVector_model)\n",
    "    effect_vector = get_mean_word_vector(effect, wordVector_model)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    if drug_vector is None or effect_vector is None:\n",
    "        return None\n",
    "\n",
    "    return cos_sim([drug_vector], [effect_vector])[0][0]\n",
    "\n",
    "def compute_cosine_similarities(drug_adr_df: pd.DataFrame, lstm_model, wordvec_model) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute cosine similarities for all drug/ADR pairs in input dataframe.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i, row in tqdm(drug_adr_df.iterrows(), total=len(drug_adr_df)):\n",
    "        sentence = Sentence(row['text'])\n",
    "        lstm_model.predict(sentence)\n",
    "\n",
    "        # Extract predicted drug and ADR entities\n",
    "        predicted_drugs = []\n",
    "        predicted_adrs = []\n",
    "        for entity in sentence.get_spans('ner'):\n",
    "            if entity.get_label('ner').value == 'DRUG':\n",
    "                predicted_drugs.append(entity.text)\n",
    "            elif entity.get_label('ner').value == 'ADR':\n",
    "                predicted_adrs.append(entity.text)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        predicted_drugs = list(set(predicted_drugs))\n",
    "        predicted_adrs = list(set(predicted_adrs))\n",
    "\n",
    "        # Compute cosine similarity for each drug, ADR pair\n",
    "        cosine_similarities = []\n",
    "        if len(predicted_drugs) > 0 and len(predicted_adrs) > 0:\n",
    "            for drug in predicted_drugs:\n",
    "                for adr in predicted_adrs:\n",
    "                    cosine_sim = get_pair_cosine_similarity(drug, adr, wordvec_model)\n",
    "                    cosine_similarities.append((drug,adr,cosine_sim))\n",
    "        \n",
    "        else:\n",
    "            cosine_similarities.append((None,None,None))\n",
    "\n",
    "        results.append({\"drug\": predicted_drugs, \"effect\": predicted_adrs, \"cosine_similarities\": cosine_similarities})\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "\"\"\"Dependency Parsing functions\"\"\"\n",
    "def _identify_noun_phrases(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    Identify Noun Phrases in input sentence.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Pattern for noun phrases: optional determiner, adjectives, nouns\n",
    "    NP_grammar = r'NP: {<DT>?<JJ>*<NN|NNS|NNP|NNPS>+}'\n",
    "\n",
    "    chunk_parser = nltk.RegexpParser(NP_grammar)\n",
    "    results = chunk_parser.parse(pos_tags)\n",
    "\n",
    "    phrases = []\n",
    "    for tree in results.subtrees():\n",
    "        if tree.label() == \"NP\":\n",
    "            words = [w for w, _ in tree.leaves()]\n",
    "\n",
    "            phrases.append(\" \".join(words))\n",
    "    \n",
    "    return phrases\n",
    "\n",
    "def _cross_ref_database(noun_phrases: List[str], drug_database: List[str], adr_database: List[str]) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Extract noun phrases that are present in drug and ADR databases.\n",
    "    \"\"\"\n",
    "    # case insensitive\n",
    "    drugs = [np.lower() for np in noun_phrases if any(np.lower() in db_drug for db_drug in drug_database)]\n",
    "    adrs = [np.lower() for np in noun_phrases if any(np.lower() in db_adr for db_adr in adr_database)]\n",
    "\n",
    "    return list(set(drugs)), list(set(adrs))\n",
    "\n",
    "\n",
    "def identify_drug_adr_dp(sentence: str, drug_database: List[str], adr_database: List[str]) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Extract drug/ADR pairs using regex pattern matching on causal related grammatical structures.\n",
    "    \"\"\"\n",
    "    # List of candidate causal terms\n",
    "    causal_terms = ['caused', 'causes', 'cause', 'causing', \n",
    "                    'experience', 'experienced', 'experiences', 'expriencing',\n",
    "                    'induced', 'induces', 'inducing', 'induce',\n",
    "                    'led', 'leading', 'leads to',\n",
    "                    'correlate', 'correlated', 'correlates', 'correlating',\n",
    "                    'associated', 'associate', 'associates', 'associating',\n",
    "                    'resulted', 'result', 'results', 'resulting',\n",
    "                    'due to', 'because of']\n",
    "    \n",
    "    # regex pattern to match causal grammatical structures\n",
    "    causal_pattern = '|'.join([re.escape(c) for c in causal_terms])\n",
    "    regex_pattern = r'(?P<NP1>\\b\\w+(?: \\w+)*\\b)\\s+(?:' + causal_pattern + r')\\s+(?P<NP2>\\b\\w+(?: \\w+)*\\b)'\n",
    "\n",
    "    # Extract all NPs\n",
    "    matches = re.finditer(regex_pattern, sentence, re.IGNORECASE)\n",
    "    nps = []\n",
    "    for match in matches:\n",
    "        nps.extend(_identify_noun_phrases(match.group('NP1')))\n",
    "        nps.extend(_identify_noun_phrases(match.group('NP2')))\n",
    "\n",
    "    # Cross-reference with drug/ADR database to identify drug and ADR pairs\n",
    "    if len(nps) > 0:\n",
    "        drugs, adrs = _cross_ref_database(nps, drug_database, adr_database)        \n",
    "    \n",
    "    else:\n",
    "        drugs, adrs = [], []\n",
    "\n",
    "    return drugs, adrs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1f3d65",
   "metadata": {},
   "source": [
    "# Collate drug and ADR dabase\n",
    "Obtain drug/ADR names from FAERs and CVP databases from the last 10 years. Also include drug/ADR names found in the HuggingFace training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760a4225",
   "metadata": {},
   "source": [
    "## FAERS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b96856a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [01:58<00:00,  2.75s/it]\n",
      "100%|██████████| 43/43 [00:36<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# Concatenate FAERS data \n",
    "faers_dir = \"data/FAERS\"\n",
    "faers_drugname_files = [os.path.join(faers_dir, file) for file in os.listdir(faers_dir) if file.startswith(\"DRUG\")]\n",
    "faers_reaction_files = [os.path.join(faers_dir, file) for file in os.listdir(faers_dir) if file.startswith(\"REAC\")]\n",
    "\n",
    "faers_drugname_df = []\n",
    "for file in tqdm(faers_drugname_files):\n",
    "    faers_drugname_df.append(pd.read_csv(file, sep='$', usecols=['primaryid', 'drugname']))\n",
    "faers_drugname_df = pd.concat(faers_drugname_df)\n",
    "\n",
    "faers_reaction_df = []\n",
    "for file in tqdm(faers_reaction_files):\n",
    "    faers_reaction_df.append(pd.read_csv(file, sep='$', usecols=['primaryid', 'drug_rec_act']))\n",
    "faers_reaction_df = pd.concat(faers_reaction_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "205892bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove leading/trailing whitespaces and end-punctuations, lowercase all text\n",
    "drugname_df = faers_drugname_df.map(lambda x: x.strip().rstrip(string.punctuation).lower() if isinstance(x, str) else x)\n",
    "reaction_df = faers_reaction_df.map(lambda x: x.strip().rstrip(string.punctuation).lower() if isinstance(x, str) else x)\n",
    "\n",
    "# Drop duplicates\n",
    "drugname_df = drugname_df.drop_duplicates()\n",
    "reaction_df = reaction_df.drop_duplicates()\n",
    "\n",
    "# Merge drugname and ADR data on primaryid\n",
    "faers_df = pd.merge(drugname_df, reaction_df, on='primaryid', how='inner')\n",
    "\n",
    "# Discard incomplete records\n",
    "faers_df = faers_df.dropna(subset=['drugname', 'drug_rec_act'])\n",
    "\n",
    "# Drop duplicates\n",
    "faers_df = faers_df.drop_duplicates()\n",
    "\n",
    "# Write out extracted FAERS data\n",
    "faers_df.to_csv(\"data/FAERS/FAERS_train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e0b476",
   "metadata": {},
   "source": [
    "## CVP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27bb9d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate CVP data \n",
    "cvp_dir = \"data/CVP\"\n",
    "cvp_drug_file = os.path.join(cvp_dir, \"report_drug.txt\")\n",
    "cvp_reaction_file = os.path.join(cvp_dir, \"reactions.txt\")\n",
    "\n",
    "cvp_drug_df = pd.read_csv(cvp_drug_file, sep='$', header=None, usecols=[1, 3], names=[\"report_id\", \"drugname\"])\n",
    "cvp_reaction_df = pd.read_csv(cvp_reaction_file, sep='$', header = None, usecols=[1,5], names =[\"report_id\", \"adr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d481044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge drugname and ADR based on ID\n",
    "cvp_df = pd.merge(cvp_drug_df, cvp_reaction_df, on='report_id', how='inner')\n",
    "cvp_df = cvp_df.drop_duplicates()\n",
    "\n",
    "# Remove leading/trailing whitespaces and lowercase all text\n",
    "cvp_df = cvp_df.map(lambda x: x.strip().lower() if isinstance(x, str) else x)\n",
    "\n",
    "# Discard incomplete records\n",
    "cvp_df = cvp_df.dropna(subset=['drugname', 'adr'])\n",
    "\n",
    "# Drop duplicates\n",
    "cvp_df = cvp_df.drop_duplicates()\n",
    "\n",
    "# Write out extracted CVP data\n",
    "cvp_df.to_csv(\"data/CVP/CVP_train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7378fe66",
   "metadata": {},
   "source": [
    "## Obtain databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236cec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather database of drug names and ADR terms\n",
    "huggingFace_train_data = pd.read_csv(\"data/HuggingFace/train_split.csv\", usecols=['drug', 'effect'])\n",
    "faers_train_data = pd.read_csv(\"data/FAERS/FAERS_train_data.csv\", usecols=['drugname' ,'drug_rec_act'])\n",
    "cvp_train_data = pd.read_csv(\"data/CVP/CVP_train_data.csv\", usecols=['drugname', 'adr'])\n",
    "\n",
    "drug_database = set(huggingFace_train_data['drug']).union(set(faers_train_data['drugname'])).union(set(cvp_train_data['drugname']))\n",
    "adr_database = set(huggingFace_train_data['effect']).union(set(faers_train_data['drug_rec_act'])).union(set(cvp_train_data['adr']))\n",
    "\n",
    "# Save drug and ADR databases as txt files\n",
    "with open(\"data/drug_database.txt\", 'w', encoding='utf-8') as f:\n",
    "    for drug in drug_database:\n",
    "        f.write(f\"{drug}\\n\")\n",
    "with open(\"data/adr_database.txt\", 'w', encoding='utf-8') as f:\n",
    "    for adr in adr_database:\n",
    "        f.write(f\"{adr}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819375f5",
   "metadata": {},
   "source": [
    "# Sequence tag data for LSTM training\n",
    "Sequence tag HuggingFace test data with IOB tags in preparation for LSTM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26b25cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HuggingFace data \n",
    "hf_df = pd.read_parquet(\"hf://datasets/ade-benchmark-corpus/ade_corpus_v2/Ade_corpus_v2_drug_ade_relation/train-00000-of-00001.parquet\")\n",
    "\n",
    "# Clean strings in data\n",
    "for col in hf_df.select_dtypes(include='object').columns:\n",
    "    hf_df[col] = hf_df[col].apply(clean_string)\n",
    "\n",
    "# Shuffle data, and split into a 70/15/15 train/val/test split\n",
    "train_df, temp_df = train_test_split(hf_df, test_size=0.3, random_state=45)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=45)\n",
    "\n",
    "hf_data = [(train_df, \"train\"), (val_df, \"val\"), (test_df, \"test\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1666dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4774/4774 [00:08<00:00, 556.96it/s]\n",
      "100%|██████████| 1023/1023 [00:01<00:00, 549.12it/s]\n",
      "100%|██████████| 1024/1024 [00:01<00:00, 752.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# Sequence tag HuggingFace data using provided start/end indexes\n",
    "for df, split in hf_data:\n",
    "    sequence_tagged_data = []\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        indexes = row[\"indexes\"]\n",
    "        drug_spans = [(s, e) for s, e in zip(indexes['drug']['start_char'], indexes['drug']['end_char'])]\n",
    "        adr_spans = [(s, e) for s, e in zip(indexes['effect']['start_char'], indexes['effect']['end_char'])]\n",
    "        \n",
    "        word_tag_pairs = sequence_tag_text(row['drug'], row['effect'], row['text'], drug_spans=drug_spans, adr_spans=adr_spans, detect_spans=False)\n",
    "        sequence_tagged_data.append(word_tag_pairs)\n",
    "\n",
    "    # Write out sequence tagged HuggingFace data\n",
    "    save_sequence_tagged_data(sequence_tagged_data, output_dir='data', output_filename=f'huggingFace_seqtag_{split}_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b0d4b0",
   "metadata": {},
   "source": [
    "# Train & Evaluate base LSTM\n",
    "Using sequence tagged data from HuggingFace, we train a LSTM model to predict drug and ADR pairs from input text. Validation and testing is done on sequence tagged validation and testing split from HuggingFace data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468a488c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d1da003a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-22 17:21:59,942 Reading data from data/HuggingFace\n",
      "2025-11-22 17:21:59,961 Train: data/HuggingFace/huggingFace_seqtag_train_data.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-22 17:21:59,970 Dev: data/HuggingFace/huggingFace_seqtag_val_data.txt\n",
      "2025-11-22 17:21:59,976 Test: data/HuggingFace/huggingFace_seqtag_test_data.txt\n",
      "2025-11-22 17:22:09,820 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "4774it [00:00, 20235.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-22 17:22:10,193 Dictionary created for label 'ner' with 2 values: ADR (seen 4856 times), DRUG (seen 4440 times)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary with 2 tags: ADR, DRUG\n"
     ]
    }
   ],
   "source": [
    "# Create FLAIR corpus from datasets\n",
    "columns = {0: \"text\", 1: \"ner\"}\n",
    "corpus = ColumnCorpus(data_folder=\"data/HuggingFace\", column_format=columns,\n",
    "                      train_file=\"huggingFace_seqtag_train_data.txt\", test_file=\"huggingFace_seqtag_test_data.txt\", dev_file=\"huggingFace_seqtag_val_data.txt\")\n",
    "\n",
    "# Label column to predict\n",
    "label_type = \"ner\"\n",
    "\n",
    "# Create label dictionary from corpus\n",
    "label_dict = corpus.make_label_dictionary(\n",
    "    label_type=label_type, add_unk=False\n",
    ")\n",
    "print(label_dict)\n",
    "\n",
    "# Initialize embedding stack with Flair and GloVe\n",
    "embedding_types = [\n",
    "    WordEmbeddings(\"glove\"),\n",
    "    FlairEmbeddings(\"news-forward\"),\n",
    "    FlairEmbeddings(\"news-backward\"),\n",
    "]\n",
    "embeddings = StackedEmbeddings(embeddings=embedding_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cced43a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-18 22:37:59,886 SequenceTagger predicts: Dictionary with 9 tags: O, S-ADR, B-ADR, E-ADR, I-ADR, S-DRUG, B-DRUG, E-DRUG, I-DRUG\n"
     ]
    }
   ],
   "source": [
    "# Initialize sequence tagger and trainer\n",
    "tagger = SequenceTagger(\n",
    "    hidden_size=256,\n",
    "    embeddings=embeddings,\n",
    "    tag_dictionary=label_dict,\n",
    "    tag_type=label_type,\n",
    ")\n",
    "trainer = ModelTrainer(tagger, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e4450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "runname = \"base-model\"\n",
    "output_dir = f\"taggers/{runname}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "trainer.train(\n",
    "    f\"taggers/{runname}\",\n",
    "    learning_rate=0.1,\n",
    "    mini_batch_size=3,\n",
    "    save_model_each_k_epochs=10,\n",
    "    monitor_test=True,\n",
    "    max_epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ed6082",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf930417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained lstm model\n",
    "trained_lstm_model = SequenceTagger.load(\n",
    "    'taggers/base-model/best-model.pt'\n",
    ")\n",
    "\n",
    "# Evaluate dependency parsing results\n",
    "test_df = pd.read_csv(\"data/HuggingFace/test_split.csv\")\n",
    "actual_drugs = []\n",
    "actual_adrs = []\n",
    "for i, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    actual_drugs.append([row['drug']])\n",
    "    actual_adrs.append([row['effect']])\n",
    "\n",
    "test_results = []\n",
    "for i, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    sentence = Sentence(row['text'])\n",
    "    trained_lstm_model.predict(sentence)\n",
    "\n",
    "    # Extract predicted drug and ADR entities\n",
    "    predicted_drugs = []\n",
    "    predicted_adrs = []\n",
    "    for entity in sentence.get_spans('ner'):\n",
    "        if entity.get_label('ner').value == 'DRUG':\n",
    "            predicted_drugs.append(entity.text)\n",
    "        elif entity.get_label('ner').value == 'ADR':\n",
    "            predicted_adrs.append(entity.text)\n",
    "\n",
    "    test_results.append({\"drug\": list(set(predicted_drugs)), \"effect\": list(set(predicted_adrs))})\n",
    "\n",
    "test_results = pd.DataFrame(test_results)\n",
    "test_results.to_csv(\"data/evaluation_results/test_split_LSTM_results.csv\", index=False)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "evaluation_metrics = get_evaluation_metrics(\n",
    "    pred_drug=test_results['drug'],\n",
    "    true_drug=actual_drugs,\n",
    "    pred_adr=test_results['effect'],\n",
    "    true_adr=actual_adrs\n",
    ")\n",
    "evaluation_metrics.to_csv(\"data/evaluation_results/eval_LSTM.csv\", index=False)\n",
    "print(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3ae58c",
   "metadata": {},
   "source": [
    "# Improving false positives\n",
    "Generate cosine similarity scores of LSTM identified drug, ADR pairs. Pairs are discarded as false positives if the cosine similarity score is < 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f5e9a8",
   "metadata": {},
   "source": [
    "## Build word vector model from training data\n",
    "Tokenize text and use it to train a word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c8cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spaCy to tokenize text in training data\n",
    "train_df = pd.read_csv(\"data/HuggingFace/train_split.csv\")\n",
    "nlp = spacy.load(\n",
    "    \"en_core_web_sm\", disable=[\"ner\", \"parser\"]\n",
    ")\n",
    "\n",
    "collection = [\n",
    "    [\n",
    "        token.text\n",
    "        for token in nlp(line.lower())\n",
    "        if not token.is_punct\n",
    "    ]\n",
    "    for line in tqdm(train_df[\"text\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d76978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model using tokenized text\n",
    "wordVec_model = Word2Vec(\n",
    "    sentences=collection,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=1,\n",
    "    negative=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272cd717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save word vectors\n",
    "word_vectors = wordVec_model.wv\n",
    "word_vectors.save(\"data/word2vec_vectors.wordvectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401897cc",
   "metadata": {},
   "source": [
    "## Assess cosine similarity threshold\n",
    "Assess the feasibility of using cosine similarities to identify true drug/ADR pairs using training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1456c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wordVector/LSTM model and training data\n",
    "trained_lstm_model = SequenceTagger.load(\n",
    "    'taggers/base-model/best-model.pt'\n",
    ")\n",
    "wordVec_model = Word2Vec.load(\"data/word2vec.model\")\n",
    "train_df = pd.read_csv(\"data/HuggingFace/train_split.csv\")\n",
    "train_cosine_similarities = compute_cosine_similarities(train_df, trained_lstm_model, wordVec_model)\n",
    "\n",
    "# Save cosine similarities\n",
    "train_cosine_similarities.to_csv(\"data/train_split_cosine_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd9f49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cosine similarities distribution\n",
    "hist_data = []\n",
    "for i, row in tqdm(train_cosine_similarities.iterrows(), total=len(train_cosine_similarities)):\n",
    "  for c in row['cosine_similarities']:\n",
    "    if c[0] and len(c[0]) > 0 and c[1] and len(c[1]) > 1:\n",
    "      if c[2] is not None:\n",
    "        hist_data.append(c[2])\n",
    "\n",
    "plt.hist(hist_data, bins=50)\n",
    "plt.title('Cosine Similarities Distribution (Training Data))')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.savefig(\"data/evaluation_results/cosine_hist_train_split.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbc58b8",
   "metadata": {},
   "source": [
    "## Incorporate cosine similarity in LSTM predictions\n",
    "Discard any drug/ADR predictions with cosine similarity < 0.4 as false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8270ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained LSTM and word vector models\n",
    "trained_lstm_model = SequenceTagger.load(\n",
    "    'taggers/base-model/best-model.pt'\n",
    ")\n",
    "wordVec_model = Word2Vec.load(\"data/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d1c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement cosine similarity for test data\n",
    "test_df = pd.read_csv(\"data/HuggingFace/test_split.csv\")\n",
    "\n",
    "cosine_results = []\n",
    "for i, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    sentence = Sentence(row['text'])\n",
    "    trained_lstm_model.predict(sentence)\n",
    "\n",
    "    # Extract LSTM predicted drug and ADR entities\n",
    "    predicted_drugs = []\n",
    "    predicted_adrs = []\n",
    "    for entity in sentence.get_spans('ner'):\n",
    "        if entity.get_label('ner').value == 'DRUG':\n",
    "            predicted_drugs.append(entity.text)\n",
    "        elif entity.get_label('ner').value == 'ADR':\n",
    "            predicted_adrs.append(entity.text)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    if len(predicted_drugs) > 0 or len(predicted_adrs) > 0:\n",
    "        drugs = []\n",
    "        adrs = []\n",
    "        for drug in predicted_drugs:\n",
    "            for adr in predicted_adrs:\n",
    "                cosine_similarity = get_pair_cosine_similarity(drug, adr, wordVec_model)\n",
    "\n",
    "                # Keep LSTM prediction if more than 0.4 similarity\n",
    "                if cosine_similarity and cosine_similarity >= 0.4:\n",
    "                    drugs.append(drug)\n",
    "                    adrs.append(adr) \n",
    "                \n",
    "                # Keep LSTM prediction if None found\n",
    "                if cosine_similarity is None:\n",
    "                    drugs.append(drug)\n",
    "                    adrs.append(adr)\n",
    "    \n",
    "    # Keep LSTM predictions if no cosine similarity available\n",
    "    else:\n",
    "        drugs = predicted_drugs\n",
    "        adrs = predicted_adrs\n",
    "    \n",
    "    cosine_results.append({\"drug\": list(set(drugs)), \"effect\": list(set(adrs))})\n",
    "\n",
    "cosine_results = pd.DataFrame(cosine_results)\n",
    "cosine_results.to_csv(\"data/evaluation_results/test_split_LSTM_cosine_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410dc1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate final results\n",
    "test_df = pd.read_csv(\"data/HuggingFace/test_split.csv\")\n",
    "actual_drugs = []\n",
    "actual_adrs = []\n",
    "for i, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    actual_drugs.append([row['drug']])\n",
    "    actual_adrs.append([row['effect']])\n",
    "\n",
    "final_results = pd.read_csv(\"data/evaluation_results/test_split_LSTM_cosine_results.csv\")\n",
    "predicted_drugs = [ast.literal_eval(drug) for drug in final_results['drug']]\n",
    "predicted_adrs = [ast.literal_eval(adr) for adr in final_results['effect']]\n",
    "eval_metrics = get_evaluation_metrics(\n",
    "    pred_drug=predicted_drugs,\n",
    "    true_drug=actual_drugs,\n",
    "    pred_adr=predicted_adrs,\n",
    "    true_adr=actual_adrs\n",
    ")\n",
    "eval_metrics.to_csv(\"data/evaluation_results/eval_cosine_corrected.csv\", index=False)\n",
    "\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5403467d",
   "metadata": {},
   "source": [
    "# Improving true positives\n",
    "Implement dependency parsing to extract drug/ADR pairs that the LSTM might have missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf32cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained LSTM and word vector models\n",
    "trained_lstm_model = SequenceTagger.load(\n",
    "    'taggers/base-model/best-model.pt'\n",
    ")\n",
    "wordVec_model = Word2Vec.load(\"data/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a034907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement dependency parsing for test data\n",
    "test_df = pd.read_csv(\"data/HuggingFace/test_split.csv\")\n",
    "with open(\"data/drug_database.txt\", 'r', encoding='utf-8') as f:\n",
    "    drug_database = [line for line in f.readlines()]\n",
    "with open(\"data/adr_database.txt\", 'r', encoding='utf-8') as f:\n",
    "    adr_database = [line for line in f.readlines()]\n",
    "\n",
    "results = []\n",
    "for i, row in tqdm(test_df.iterrows(), total = len(test_df)):\n",
    "    sentence = Sentence(row['text'])\n",
    "    trained_lstm_model.predict(sentence)\n",
    "\n",
    "    # Extract LSTM predicted drug and ADR entities\n",
    "    drugs = []\n",
    "    adrs = []\n",
    "    for entity in sentence.get_spans('ner'):\n",
    "        if entity.get_label('ner').value == 'DRUG':\n",
    "            drugs.append(entity.text)\n",
    "        elif entity.get_label('ner').value == 'ADR':\n",
    "            adrs.append(entity.text)\n",
    "\n",
    "    # Add dependency parsing matches if LSTM has no predictions\n",
    "    dp_drugs, dp_adrs = identify_drug_adr_dp(row['text'], drug_database, adr_database)\n",
    "    if len(drugs) == 0:\n",
    "        drugs = dp_drugs\n",
    "    if len(adrs) == 0:\n",
    "        adrs = dp_adrs\n",
    "\n",
    "    results.append({\n",
    "        'drug': list(set(drugs)),\n",
    "        'effect': list(set(adrs)),\n",
    "    })\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results.to_csv(\"data/evaluation_results/test_split_LSTM_and_DP_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1006b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate dependency parsing results\n",
    "test_df = pd.read_csv(\"data/HuggingFace/test_split.csv\")\n",
    "actual_drugs = []\n",
    "actual_adrs = []\n",
    "for i, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    actual_drugs.append([row['drug']])\n",
    "    actual_adrs.append([row['effect']])\n",
    "\n",
    "final_results = pd.read_csv(\"data/evaluation_results/test_split_LSTM_and_DP_results.csv\")\n",
    "predicted_drugs = [ast.literal_eval(drug) for drug in final_results['drug']]\n",
    "predicted_adrs = [ast.literal_eval(adr) for adr in final_results['effect']]\n",
    "eval_metrics = get_evaluation_metrics(\n",
    "    pred_drug=predicted_drugs,\n",
    "    true_drug=actual_drugs,\n",
    "    pred_adr=predicted_adrs,\n",
    "    true_adr=actual_adrs\n",
    ")\n",
    "eval_metrics.to_csv(\"data/evaluation_results/eval_DP_corrected.csv\", index=False)\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6f41f",
   "metadata": {},
   "source": [
    "# Integrate improvements into LSTM model\n",
    "Use LSTM to make base predictions. Discard predictions using cosine similarity, then use dependency parsing to supplement discarded/missed predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a856cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained LSTM and word vector models\n",
    "trained_lstm_model = SequenceTagger.load(\n",
    "    'taggers/base-model/best-model.pt'\n",
    ")\n",
    "wordVec_model = Word2Vec.load(\"data/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f3059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "test_df = pd.read_csv(\"data/HuggingFace/test_split.csv\")\n",
    "with open(\"data/drug_database.txt\", 'r', encoding='utf-8') as f:\n",
    "    drug_database = [line for line in f.readlines()]\n",
    "with open(\"data/adr_database.txt\", 'r', encoding='utf-8') as f:\n",
    "    adr_database = [line for line in f.readlines()]\n",
    "\n",
    "final_results = []\n",
    "for i, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    sentence = Sentence(row['text'])\n",
    "    trained_lstm_model.predict(sentence)\n",
    "\n",
    "    # Extract LSTM predicted drug and ADR entities\n",
    "    predicted_drugs = []\n",
    "    predicted_adrs = []\n",
    "    for entity in sentence.get_spans('ner'):\n",
    "        if entity.get_label('ner').value == 'DRUG':\n",
    "            predicted_drugs.append(entity.text)\n",
    "        elif entity.get_label('ner').value == 'ADR':\n",
    "            predicted_adrs.append(entity.text)\n",
    "\n",
    "    dp_drugs, dp_adrs = identify_drug_adr_dp(row['text'], drug_database, adr_database)\n",
    "    # If LSTM has predictions\n",
    "    if len(predicted_drugs) > 0 and len(predicted_adrs) > 0:\n",
    "        drugs = []\n",
    "        adrs = []\n",
    "\n",
    "        for drug in predicted_drugs:\n",
    "            for adr in predicted_adrs:\n",
    "                cosine_similarity = get_pair_cosine_similarity(drug, adr, wordVec_model)\n",
    "\n",
    "                # keep LSTM prediction if more than 0.4 similarity, or None\n",
    "                if cosine_similarity and cosine_similarity > 0.4:\n",
    "                    drugs.append(drug)\n",
    "                    adrs.append(adr)\n",
    "\n",
    "                elif cosine_similarity is None:\n",
    "                    drugs.append(drug)\n",
    "                    adrs.append(adr)               \n",
    "\n",
    "        # Use dependency parsing if all pairs discarded by cosine similarity  \n",
    "        # Retain LSTM predictions if no dependency parsing available        \n",
    "        if len(drugs) == 0 and len(dp_drugs) > 0:\n",
    "            drugs = dp_drugs\n",
    "        else:\n",
    "            drugs = predicted_drugs\n",
    "\n",
    "        if len(adrs) == 0 and len(dp_adrs) > 0:\n",
    "            adrs = dp_adrs\n",
    "        else:\n",
    "            adrs = predicted_adrs\n",
    "    \n",
    "    # If LSTM has no predictions or missing half of the pair, use dependency parsing results\n",
    "    else:\n",
    "        if len(predicted_drugs) == 0:\n",
    "            drugs = dp_drugs\n",
    "        else:\n",
    "            drugs = predicted_drugs\n",
    "        \n",
    "        if len(predicted_adrs) == 0:\n",
    "            adrs = dp_adrs\n",
    "        else:\n",
    "            adrs = predicted_adrs\n",
    "    \n",
    "    final_results.append({\n",
    "        'drug': list(set(drugs)),\n",
    "        'effect': list(set(adrs)),\n",
    "    })\n",
    "\n",
    "final_results = pd.DataFrame(final_results)\n",
    "final_results.to_csv(\"data/evaluation_results/test_split_finalimprov_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d29110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate final results\n",
    "test_df = pd.read_csv(\"data/HuggingFace/test_split.csv\")\n",
    "actual_drugs = []\n",
    "actual_adrs = []\n",
    "for i, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    actual_drugs.append([row['drug']])\n",
    "    actual_adrs.append([row['effect']])\n",
    "\n",
    "final_results = pd.read_csv(\"data/evaluation_results/test_split_finalimprov_results.csv\")\n",
    "predicted_drugs = [ast.literal_eval(drug) for drug in final_results['drug']]\n",
    "predicted_adrs = [ast.literal_eval(adr) for adr in final_results['effect']]\n",
    "eval_metrics = get_evaluation_metrics(\n",
    "    pred_drug=predicted_drugs,\n",
    "    true_drug=actual_drugs,\n",
    "    pred_adr=predicted_adrs,\n",
    "    true_adr=actual_adrs\n",
    ")\n",
    "eval_metrics.to_csv(\"data/evaluation_results/eval_finalimprov.csv\", index=False)\n",
    "\n",
    "print(eval_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
